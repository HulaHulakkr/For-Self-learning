\section{Discussion}
We have presented a novel loss, inspired by contrastive learning, that outperforms cross entropy on classification accuracy and robustness benchmarks. Furthermore, our experiments show that this loss is less sensitive to hyperparameter changes, which is a useful practical consideration. The loss function provides a natural connection between fully unsupervised training on the one end, and fully supervised training on the other. This opens the possibility of applications in semi-supervised learning which can leverage the benefits of a single loss that can smoothly shift behavior based on the availability of labeled data.