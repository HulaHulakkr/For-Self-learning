\section{Related Work}
\label{sec:related}
Our work draws on existing literature in self-supervised representation learning, metric learning and supervised learning. Here we focus on the most relevant papers. The cross-entropy loss was introduced as a powerful loss function to train deep networks \cite{rumelhart1986learning,baum1988supervised,levin1988accelerated}. The key idea is simple and intuitive: each class is assigned a target (usually 1-hot) vector. However, it is unclear why these target labels should be the optimal ones and some work has tried to identify better target label vectors, e.g. \cite{yang2015deep}. A number of papers have studied other drawbacks of the cross-entropy loss, such as sensitivity to noisy labels \cite{zhang2018generalized,sukhbaatar2014training}, presence of adversarial examples \cite{elsayed2018large,nar2019cross}, and poor margins \cite{cao2019learning}. Alternative losses have been proposed, but the most effective ideas in practice have been approaches that change the reference label distribution, such as label smoothing \cite{szegedy2016rethinking,muller2019does}, data augmentations such as Mixup \cite{zhang2017mixup} and CutMix \cite{yun2019cutmix}, and knowledge distillation \cite{hinton2015distilling}.

Powerful self-supervised representation learning approaches based on deep learning models have recently been developed in the natural language domain \cite{devlin2018bert,yang2019xlnet,mikolov2013distributed}. In the image domain, pixel-predictive approaches have also been used to learn embeddings \cite{doersch2015unsupervised,zhang2016colorful,zhang2017split,noroozi2016unsupervised}. These methods try to predict missing parts of the input signal. However, a more effective approach has been to replace a dense per-pixel predictive loss, with a loss in lower-dimensional representation space. The state of the art family of models for self-supervised representation learning using this paradigm are collected under the umbrella of contrastive learning \cite{wu2018unsupervised,henaff2019data,hjelm2018learning,tian2019contrastive,sermanet2017time,chen2020simple,tschannen2019mutual}. In these works, the losses are inspired by noise contrastive estimation \cite{gutmann2010noise,mnih2013learning} or N-pair losses \cite{sohn2016improved}. Typically, the loss is applied at the last layer of a deep network. At test time, the embeddings from a previous layer are utilized for downstream transfer tasks, fine tuning or direct retrieval tasks. \cite{he2019momentum} introduces the approximation of only back-propagating through part of the loss, and also the approximation of using stale representations in the form of a memory bank.

Closely related to contrastive learning is the family of losses based on metric distance learning or triplets \cite{chopra2005learning,weinberger2009distance,schroff2015facenet}. These losses have been used to learn powerful representations, often in supervised settings, where labels are used to guide the choice of positive and negative pairs. The key distinction between triplet losses and contrastive losses is the number of positive and negative pairs per data point; triplet losses use exactly one positive and one negative pair per anchor. In the supervised metric learning setting, the positive pair is chosen from the same class and the negative pair is chosen from other classes, nearly always requiring hard-negative mining for good performance \cite{schroff2015facenet}. Self-supervised contrastive losses similarly use just one positive pair for each anchor sample, selected using either co-occurrence \cite{henaff2019data,hjelm2018learning,tian2019contrastive} or data augmentation \cite{chen2020simple}. The major difference is that many negative pairs are used for each anchor. These are usually chosen uniformly at random using some form of weak knowledge, such as patches from other images, or frames from other randomly chosen videos, relying on the assumption that this approach yields a very low probability of false negatives.

 Resembling our supervised contrastive approach is the soft-nearest neighbors loss introduced in  \cite{salakhutdinov2007learning} and used in \cite{wu2018improving}. Like \cite{wu2018improving}, we improve upon \cite{salakhutdinov2007learning} by normalizing the embeddings and replacing euclidean distance with inner products. We further improve on \cite{wu2018improving} by the increased use of data augmentation, a disposable contrastive head and two-stage training (contrastive followed by cross-entropy), and crucially, changing the form of the loss function to significantly improve results (see Section 3). \cite{frosst2019analyzing} also uses a closely related loss formulation to ours to \textit{entangle} representations at intermediate layers by maximizing the loss. Most similar to our method is the Compact Clustering via Label Propagation (CCLP) regularizer in Kamnitsas et. al. \cite{Kamnitsas2018SemiSupervisedLV}. While CCLP focuses mostly on the semi-supervised case, in the fully supervised case the regularizer reduces to almost exactly our loss formulation. Important practical differences include our normalization of the contrastive embedding onto the unit sphere, tuning of a temperature parameter in the contrastive objective, and stronger augmentation. Additionally, Kamnitsas et. al. use the contrastive embedding as an input to a classification head, which is trained jointly with the CCLP regularizer, while SupCon employs a two stage training and discards the contrastive head. Lastly, the scale of experiments in Kamnitsas et. al. is much smaller than in this work. Merging the findings of our paper and CCLP is a promising direction for semi-supervised learning research. 

