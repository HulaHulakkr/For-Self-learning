\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\citation{cubuk2019autoaugment}
\citation{cubuk2019randaugment}
\citation{yun2019cutmix}
\citation{cubuk2019autoaugment}
\citation{cubuk2019randaugment}
\citation{yun2019cutmix}
\citation{zhang2018generalized}
\citation{sukhbaatar2014training}
\citation{elsayed2018large}
\citation{liu2016large}
\citation{deng2009imagenet}
\citation{cubuk2019autoaugment}
\citation{cubuk2019randaugment}
\citation{xie2019self}
\citation{kolesnikov2019large}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Our SupCon loss consistently outperforms cross-entropy with standard data augmentations. We show top-1 accuracy for the ImageNet dataset, on ResNet-50, ResNet-101 and ResNet-200, and compare against AutoAugment \cite  {cubuk2019autoaugment}, RandAugment \cite  {cubuk2019randaugment} and CutMix \cite  {yun2019cutmix}. }}{1}{figure.caption.2}\protected@file@percent }
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{cubuk2019randaugment}{{1}{1}{figure.caption.2}}}
\@writefile{brf}{\backcite{yun2019cutmix}{{1}{1}{figure.caption.2}}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:imagenet_top1_teaser}{{1}{1}{\small Our SupCon loss consistently outperforms cross-entropy with standard data augmentations. We show top-1 accuracy for the ImageNet dataset, on ResNet-50, ResNet-101 and ResNet-200, and compare against AutoAugment \cite {cubuk2019autoaugment}, RandAugment \cite {cubuk2019randaugment} and CutMix \cite {yun2019cutmix}}{figure.caption.2}{}}
\@writefile{brf}{\backcite{zhang2018generalized,sukhbaatar2014training}{{1}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{elsayed2018large,liu2016large}{{1}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{1}{1}{figure.caption.3}}}
\citation{wu2018unsupervised}
\citation{henaff2019data}
\citation{oord2018representation}
\citation{tian2019contrastive}
\citation{hjelm2018learning}
\citation{chen2020simple}
\citation{he2019momentum}
\citation{oord2018representation}
\citation{tian2019contrastive}
\citation{weinberger2009distance}
\citation{sohn2016improved}
\citation{he2016deep}
\citation{cubuk2019autoaugment}
\citation{lim2019fast}
\citation{hendrycks2019benchmarking}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Supervised vs. self-supervised contrastive losses: The self-supervised contrastive loss (left, Eq. \ref {eqn:self_loss}) contrasts a \emph  {single} positive for each anchor (i.e., an augmented version of the same image) against a set of negatives consisting of the entire remainder of the batch. The supervised contrastive loss (right) considered in this paper (Eq. \ref {eqn:supervised_loss}), however, contrasts the set of \emph  {all} samples from the same class as positives against the negatives from the remainder of the batch. As demonstrated by the photo of the black and white puppy, taking class label information into account results in an embedding space where elements of the same class are more closely aligned than in the self-supervised case.}}{2}{figure.caption.3}\protected@file@percent }
\newlabel{fig:teaser1}{{2}{2}{\small Supervised vs. self-supervised contrastive losses: The self-supervised contrastive loss (left, Eq. \ref {eqn:self_loss}) contrasts a \emph {single} positive for each anchor (i.e., an augmented version of the same image) against a set of negatives consisting of the entire remainder of the batch. The supervised contrastive loss (right) considered in this paper (Eq. \ref {eqn:supervised_loss}), however, contrasts the set of \emph {all} samples from the same class as positives against the negatives from the remainder of the batch. As demonstrated by the photo of the black and white puppy, taking class label information into account results in an embedding space where elements of the same class are more closely aligned than in the self-supervised case}{figure.caption.3}{}}
\@writefile{brf}{\backcite{cubuk2019autoaugment,cubuk2019randaugment,xie2019self,kolesnikov2019large}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{wu2018unsupervised,henaff2019data,oord2018representation,tian2019contrastive,hjelm2018learning,chen2020simple,he2019momentum}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{oord2018representation,tian2019contrastive}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{weinberger2009distance}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{sohn2016improved}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{he2016deep}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{2}{1}{figure.caption.3}}}
\@writefile{brf}{\backcite{lim2019fast}{{2}{1}{figure.caption.3}}}
\citation{rumelhart1986learning}
\citation{baum1988supervised}
\citation{levin1988accelerated}
\citation{yang2015deep}
\citation{zhang2018generalized}
\citation{sukhbaatar2014training}
\citation{elsayed2018large}
\citation{nar2019cross}
\citation{cao2019learning}
\citation{szegedy2016rethinking}
\citation{muller2019does}
\citation{zhang2017mixup}
\citation{yun2019cutmix}
\citation{hinton2015distilling}
\citation{devlin2018bert}
\citation{yang2019xlnet}
\citation{mikolov2013distributed}
\citation{doersch2015unsupervised}
\citation{zhang2016colorful}
\citation{zhang2017split}
\citation{noroozi2016unsupervised}
\citation{wu2018unsupervised}
\citation{henaff2019data}
\citation{hjelm2018learning}
\citation{tian2019contrastive}
\citation{sermanet2017time}
\citation{chen2020simple}
\citation{tschannen2019mutual}
\citation{gutmann2010noise}
\citation{mnih2013learning}
\citation{sohn2016improved}
\citation{he2019momentum}
\citation{chopra2005learning}
\citation{weinberger2009distance}
\citation{schroff2015facenet}
\citation{schroff2015facenet}
\citation{henaff2019data}
\citation{hjelm2018learning}
\citation{tian2019contrastive}
\citation{chen2020simple}
\citation{salakhutdinov2007learning}
\citation{wu2018improving}
\citation{wu2018improving}
\citation{salakhutdinov2007learning}
\citation{wu2018improving}
\citation{frosst2019analyzing}
\citation{Kamnitsas2018SemiSupervisedLV}
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{3}{1}{figure.caption.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}\protected@file@percent }
\newlabel{sec:related}{{2}{3}{Related Work}{section.2}{}}
\@writefile{brf}{\backcite{rumelhart1986learning,baum1988supervised,levin1988accelerated}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{yang2015deep}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{zhang2018generalized,sukhbaatar2014training}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{elsayed2018large,nar2019cross}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{cao2019learning}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{szegedy2016rethinking,muller2019does}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{zhang2017mixup}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{yun2019cutmix}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{hinton2015distilling}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{devlin2018bert,yang2019xlnet,mikolov2013distributed}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{doersch2015unsupervised,zhang2016colorful,zhang2017split,noroozi2016unsupervised}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{wu2018unsupervised,henaff2019data,hjelm2018learning,tian2019contrastive,sermanet2017time,chen2020simple,tschannen2019mutual}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{gutmann2010noise,mnih2013learning}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{sohn2016improved}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{he2019momentum}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{chopra2005learning,weinberger2009distance,schroff2015facenet}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{schroff2015facenet}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{henaff2019data,hjelm2018learning,tian2019contrastive}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{chen2020simple}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{salakhutdinov2007learning}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{wu2018improving}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{wu2018improving}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{salakhutdinov2007learning}{{3}{2}{section.2}}}
\@writefile{brf}{\backcite{wu2018improving}{{3}{2}{section.2}}}
\citation{tian2019contrastive}
\citation{chen2020simple}
\citation{schroff2015facenet}
\citation{wang2020understanding}
\citation{hastie2001statisticallearning}
\citation{tian2019contrastive}
\citation{chen2020simple}
\@writefile{brf}{\backcite{frosst2019analyzing}{{4}{2}{section.2}}}
\@writefile{brf}{\backcite{Kamnitsas2018SemiSupervisedLV}{{4}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{4}{section.3}\protected@file@percent }
\@writefile{brf}{\backcite{tian2019contrastive,chen2020simple}{{4}{3}{section.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Representation Learning Framework}{4}{subsection.3.1}\protected@file@percent }
\@writefile{brf}{\backcite{schroff2015facenet,wang2020understanding}{{4}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{hastie2001statisticallearning}{{4}{3.1}{subsection.3.1}}}
\@writefile{brf}{\backcite{tian2019contrastive,chen2020simple}{{4}{3.1}{subsection.3.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Contrastive Loss Functions}{4}{subsection.3.2}\protected@file@percent }
\newlabel{sec:contrastive_losses}{{3.2}{4}{Contrastive Loss Functions}{subsection.3.2}{}}
\citation{chen2020simple}
\citation{tian2019contrastive}
\citation{henaff2019data}
\citation{hjelm2018learning}
\citation{gutmann2010noise}
\citation{sohn2016improved}
\citation{henaff2019data}
\citation{he2019momentum}
\citation{tian2019contrastive}
\citation{chen2020simple}
\citation{schroff2015facenet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Self-Supervised Contrastive Loss}{5}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{brf}{\backcite{chen2020simple,tian2019contrastive,henaff2019data,hjelm2018learning}{{5}{3.2.1}{subsubsection.3.2.1}}}
\newlabel{eqn:self_loss}{{1}{5}{Self-Supervised Contrastive Loss}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Supervised Contrastive Losses}{5}{subsubsection.3.2.2}\protected@file@percent }
\newlabel{eqn:supervised_loss}{{2}{5}{Supervised Contrastive Losses}{equation.3.2}{}}
\newlabel{eqn:bad_supervised_loss}{{3}{5}{Supervised Contrastive Losses}{equation.3.3}{}}
\@writefile{brf}{\backcite{gutmann2010noise,sohn2016improved}{{5}{3.2.2}{equation.3.3}}}
\@writefile{brf}{\backcite{henaff2019data,he2019momentum,tian2019contrastive,chen2020simple}{{5}{3.2.2}{equation.3.3}}}
\citation{jensen1906sur}
\citation{deng2009imagenet}
\citation{he2016deep}
\citation{weinberger2009distance}
\citation{sohn2016improved}
\citation{krizhevsky2009learning}
\citation{deng2009imagenet}
\citation{hendrycks2019benchmarking}
\citation{he2016deep}
\citation{cubuk2019autoaugment}
\citation{cubuk2019randaugment}
\citation{chen2020simple}
\citation{tian2020makes}
\@writefile{brf}{\backcite{schroff2015facenet}{{6}{3.2.2}{equation.3.3}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ ImageNet Top-1 classification accuracy for supervised contrastive losses on ResNet-50 for a batch size of 6144. }}{6}{table.caption.4}\protected@file@percent }
\newlabel{tab:supervised_loss_variants}{{1}{6}{\small ImageNet Top-1 classification accuracy for supervised contrastive losses on ResNet-50 for a batch size of 6144}{table.caption.4}{}}
\@writefile{brf}{\backcite{jensen1906sur}{{6}{3.2.2}{table.caption.4}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{6}{3.2.2}{table.caption.4}}}
\@writefile{brf}{\backcite{he2016deep}{{6}{3.2.2}{table.caption.4}}}
\newlabel{eqn:gradient}{{4}{6}{Supervised Contrastive Losses}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Connection to Triplet Loss and N-pairs Loss}{6}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{brf}{\backcite{weinberger2009distance}{{6}{3.2.3}{subsubsection.3.2.3}}}
\@writefile{brf}{\backcite{sohn2016improved}{{6}{3.2.3}{subsubsection.3.2.3}}}
\citation{he2016deep}
\citation{yun2019cutmix}
\citation{yun2019cutmix}
\citation{zhang2017mixup}
\citation{he2019momentum}
\citation{sohn2016improved}
\citation{sohn2016improved}
\citation{chen2020simple}
\citation{liu2016largemargin}
\citation{he2016deep}
\citation{chen2020simple}
\citation{liu2016largemargin}
\citation{he2016deep}
\citation{chen2020simple}
\citation{liu2016largemargin}
\citation{zhang2017mixup}
\citation{yun2019cutmix}
\citation{cubuk2019autoaugment}
\citation{lim2019fast}
\citation{cubuk2019autoaugment}
\citation{cubuk2019autoaugment}
\citation{tian2020makes}
\citation{tian2020makes}
\citation{tian2020makes}
\citation{cubuk2019autoaugment}
\citation{tian2020makes}
\citation{cubuk2019autoaugment}
\citation{tian2020makes}
\citation{krizhevsky2012imagenet}
\citation{Simonyan15}
\citation{he2016deep}
\citation{hendrycks2019benchmarking}
\citation{hendrycks2019benchmarking}
\citation{hendrycks2019benchmarking}
\citation{hendrycks2019benchmarking}
\@writefile{brf}{\backcite{chen2020simple}{{7}{\caption@xref {??}{ on input line 22}}{table.caption.5}}}
\@writefile{brf}{\backcite{liu2016largemargin}{{7}{\caption@xref {??}{ on input line 22}}{table.caption.5}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Top-1 classification accuracy on ResNet-50 \cite  {he2016deep} for various datasets. We compare cross-entropy training, unsupervised representation learning (SimCLR \cite  {chen2020simple}), max-margin classifiers \cite  {liu2016largemargin} and SupCon (ours). We re-implemented and tuned hyperparameters for all baseline numbers except margin classifiers where we report published results. Note that the CIFAR-10 and CIFAR-100 results are from our PyTorch implementation and ImageNet from our TensorFlow implementation.}}{7}{table.caption.5}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{7}{2}{table.caption.5}}}
\@writefile{brf}{\backcite{chen2020simple}{{7}{2}{table.caption.5}}}
\@writefile{brf}{\backcite{liu2016largemargin}{{7}{2}{table.caption.5}}}
\newlabel{tab:datasets}{{2}{7}{Top-1 classification accuracy on ResNet-50 \cite {he2016deep} for various datasets. We compare cross-entropy training, unsupervised representation learning (SimCLR \cite {chen2020simple}), max-margin classifiers \cite {liu2016largemargin} and SupCon (ours). We re-implemented and tuned hyperparameters for all baseline numbers except margin classifiers where we report published results. Note that the CIFAR-10 and CIFAR-100 results are from our PyTorch implementation and ImageNet from our TensorFlow implementation}{table.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{7}{section.4}\protected@file@percent }
\newlabel{sec:experiments}{{4}{7}{Experiments}{section.4}{}}
\@writefile{brf}{\backcite{krizhevsky2009learning}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{deng2009imagenet}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{he2016deep}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{cubuk2019randaugment}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{chen2020simple}{{7}{4}{section.4}}}
\@writefile{brf}{\backcite{tian2020makes}{{7}{4}{section.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Classification Accuracy}{7}{subsection.4.1}\protected@file@percent }
\@writefile{brf}{\backcite{he2016deep}{{7}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{yun2019cutmix}{{7}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{yun2019cutmix}{{7}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{zhang2017mixup}{{7}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{he2019momentum}{{7}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{sohn2016improved}{{7}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{sohn2016improved}{{7}{4.1}{subsection.4.1}}}
\citation{cubuk2019randaugment}
\citation{cubuk2019autoaugment}
\citation{chen2020simple}
\citation{tian2020makes}
\@writefile{brf}{\backcite{zhang2017mixup}{{8}{\caption@xref {??}{ on input line 41}}{table.caption.6}}}
\@writefile{brf}{\backcite{yun2019cutmix}{{8}{\caption@xref {??}{ on input line 42}}{table.caption.6}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{8}{\caption@xref {??}{ on input line 43}}{table.caption.6}}}
\@writefile{brf}{\backcite{lim2019fast}{{8}{\caption@xref {??}{ on input line 44}}{table.caption.6}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{8}{\caption@xref {??}{ on input line 45}}{table.caption.6}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{8}{\caption@xref {??}{ on input line 47}}{table.caption.6}}}
\@writefile{brf}{\backcite{tian2020makes}{{8}{\caption@xref {??}{ on input line 48}}{table.caption.6}}}
\@writefile{brf}{\backcite{tian2020makes}{{8}{\caption@xref {??}{ on input line 49}}{table.caption.6}}}
\@writefile{brf}{\backcite{tian2020makes}{{8}{\caption@xref {??}{ on input line 51}}{table.caption.6}}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Top-1/Top-5 accuracy results on ImageNet for AutoAugment \cite  {cubuk2019autoaugment} with ResNet-50 and for Stacked RandAugment \cite  {tian2020makes} with ResNet-101 and ResNet-200. The baseline numbers are taken from the referenced papers, and we also re-implement cross-entropy.}}{8}{table.caption.6}\protected@file@percent }
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{8}{3}{table.caption.6}}}
\@writefile{brf}{\backcite{tian2020makes}{{8}{3}{table.caption.6}}}
\newlabel{table:imagenet_top1}{{3}{8}{Top-1/Top-5 accuracy results on ImageNet for AutoAugment \cite {cubuk2019autoaugment} with ResNet-50 and for Stacked RandAugment \cite {tian2020makes} with ResNet-101 and ResNet-200. The baseline numbers are taken from the referenced papers, and we also re-implement cross-entropy}{table.caption.6}{}}
\@writefile{brf}{\backcite{krizhevsky2012imagenet}{{8}{\caption@xref {??}{ on input line 72}}{figure.caption.7}}}
\@writefile{brf}{\backcite{Simonyan15}{{8}{\caption@xref {??}{ on input line 73}}{figure.caption.7}}}
\@writefile{brf}{\backcite{he2016deep}{{8}{\caption@xref {??}{ on input line 74}}{figure.caption.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Training with supervised contrastive loss makes models more robust to corruptions in images. {\bf  Left}: Robustness as measured by Mean Corruption Error (mCE) and relative mCE over the ImageNet-C dataset \cite  {hendrycks2019benchmarking} (lower is better). {\bf  Right}: Mean Accuracy as a function of corruption severity averaged over all various corruptions. (higher is better).}}{8}{figure.caption.7}\protected@file@percent }
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{8}{3}{figure.caption.7}}}
\newlabel{table:robustness}{{3}{8}{Training with supervised contrastive loss makes models more robust to corruptions in images. {\bf Left}: Robustness as measured by Mean Corruption Error (mCE) and relative mCE over the ImageNet-C dataset \cite {hendrycks2019benchmarking} (lower is better). {\bf Right}: Mean Accuracy as a function of corruption severity averaged over all various corruptions. (higher is better)}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Robustness to Image Corruptions and Reduced Training Data}{8}{subsection.4.2}\protected@file@percent }
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{8}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{8}{4.2}{subsection.4.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Accuracy of cross-entropy and supervised contrastive loss as a function of hyperparameters and training data size, all measured on ImageNet with a ResNet-50 encoder. (From left to right) {\bf  (a)}: Standard boxplot showing Top-1 accuracy vs changes in augmentation, optimizer and learning rates. {\bf  (b)}: Top-1 accuracy as a function of batch size shows both losses benefit from larger batch sizes while Supervised Contrastive has higher Top-1 accuracy even when trained with smaller batch sizes. {\bf  (c)}: Top-1 accuracy as a function of SupCon pretraining epochs. {\bf  (d)}: Top-1 accuracy as a function of temperature during pretraining stage for SupCon. }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig:hparam_stability}{{4}{8}{Accuracy of cross-entropy and supervised contrastive loss as a function of hyperparameters and training data size, all measured on ImageNet with a ResNet-50 encoder. (From left to right) {\bf (a)}: Standard boxplot showing Top-1 accuracy vs changes in augmentation, optimizer and learning rates. {\bf (b)}: Top-1 accuracy as a function of batch size shows both losses benefit from larger batch sizes while Supervised Contrastive has higher Top-1 accuracy even when trained with smaller batch sizes. {\bf (c)}: Top-1 accuracy as a function of SupCon pretraining epochs. {\bf (d)}: Top-1 accuracy as a function of temperature during pretraining stage for SupCon}{figure.caption.8}{}}
\citation{chen2020simple}
\citation{he2019rethinking}
\citation{kornblith2019better}
\citation{chen2020simple}
\citation{pascal-voc-2007}
\citation{pascal-voc-2007}
\citation{you2017large}
\citation{hinton2012neural}
\citation{ruder2016overview}
\bibstyle{ieee_fullname}
\bibdata{supcon.bib}
\@writefile{brf}{\backcite{chen2020simple}{{9}{\caption@xref {??}{ on input line 130}}{table.caption.9}}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Transfer learning results. Numbers are mAP for VOC2007 \cite  {pascal-voc-2007}; mean-per-class accuracy for Aircraft, Pets, Caltech, and Flowers; and top-1 accuracy for all other datasets.}}{9}{table.caption.9}\protected@file@percent }
\@writefile{brf}{\backcite{pascal-voc-2007}{{9}{4}{table.caption.9}}}
\newlabel{table:transfer}{{4}{9}{Transfer learning results. Numbers are mAP for VOC2007 \cite {pascal-voc-2007}; mean-per-class accuracy for Aircraft, Pets, Caltech, and Flowers; and top-1 accuracy for all other datasets}{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Hyperparameter Stability}{9}{subsection.4.3}\protected@file@percent }
\@writefile{brf}{\backcite{cubuk2019randaugment}{{9}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{9}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{chen2020simple}{{9}{4.3}{subsection.4.3}}}
\@writefile{brf}{\backcite{tian2020makes}{{9}{4.3}{subsection.4.3}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Transfer Learning}{9}{subsection.4.4}\protected@file@percent }
\@writefile{brf}{\backcite{chen2020simple}{{9}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{he2019rethinking}{{9}{4.4}{subsection.4.4}}}
\@writefile{brf}{\backcite{kornblith2019better}{{9}{4.4}{subsection.4.4}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Training Details}{9}{subsection.4.5}\protected@file@percent }
\bibcite{baum1988supervised}{1}
\bibcite{cao2019learning}{2}
\bibcite{chen2020simple}{3}
\bibcite{chopra2005learning}{4}
\bibcite{cubuk2019autoaugment}{5}
\bibcite{cubuk2019randaugment}{6}
\bibcite{deng2009imagenet}{7}
\bibcite{devlin2018bert}{8}
\bibcite{doersch2015unsupervised}{9}
\bibcite{elsayed2018large}{10}
\bibcite{pascal-voc-2007}{11}
\bibcite{frosst2019analyzing}{12}
\bibcite{gutmann2010noise}{13}
\bibcite{hastie2001statisticallearning}{14}
\bibcite{he2019momentum}{15}
\@writefile{brf}{\backcite{you2017large}{{10}{4.5}{subsection.4.5}}}
\@writefile{brf}{\backcite{hinton2012neural}{{10}{4.5}{subsection.4.5}}}
\@writefile{brf}{\backcite{ruder2016overview}{{10}{4.5}{subsection.4.5}}}
\bibcite{he2019rethinking}{16}
\bibcite{he2016deep}{17}
\bibcite{henaff2019data}{18}
\bibcite{hendrycks2019benchmarking}{19}
\bibcite{hinton2012neural}{20}
\bibcite{hinton2015distilling}{21}
\bibcite{hjelm2018learning}{22}
\bibcite{jensen1906sur}{23}
\bibcite{Kamnitsas2018SemiSupervisedLV}{24}
\bibcite{kolesnikov2019large}{25}
\bibcite{kornblith2019better}{26}
\bibcite{krizhevsky2009learning}{27}
\bibcite{krizhevsky2012imagenet}{28}
\bibcite{levin1988accelerated}{29}
\bibcite{lim2019fast}{30}
\bibcite{liu2016large}{31}
\bibcite{liu2016largemargin}{32}
\bibcite{mikolov2013distributed}{33}
\bibcite{mnih2013learning}{34}
\bibcite{muller2019does}{35}
\bibcite{nar2019cross}{36}
\bibcite{noroozi2016unsupervised}{37}
\bibcite{oord2018representation}{38}
\bibcite{ruder2016overview}{39}
\bibcite{rumelhart1986learning}{40}
\bibcite{salakhutdinov2007learning}{41}
\bibcite{schroff2015facenet}{42}
\bibcite{sermanet2017time}{43}
\bibcite{Simonyan15}{44}
\bibcite{sohn2016improved}{45}
\bibcite{sukhbaatar2014training}{46}
\bibcite{szegedy2016rethinking}{47}
\bibcite{tian2019contrastive}{48}
\bibcite{tian2020makes}{49}
\bibcite{tieleman2012lecture}{50}
\bibcite{tschannen2019mutual}{51}
\bibcite{wang2020understanding}{52}
\bibcite{weinberger2009distance}{53}
\bibcite{wu2018improving}{54}
\bibcite{wu2018unsupervised}{55}
\bibcite{xie2019self}{56}
\bibcite{yang2015deep}{57}
\bibcite{yang2019xlnet}{58}
\bibcite{you2017large}{59}
\bibcite{yun2019cutmix}{60}
\bibcite{zhang2017mixup}{61}
\bibcite{zhang2016colorful}{62}
\bibcite{zhang2017split}{63}
\bibcite{zhang2018generalized}{64}
\citation{wu2018unsupervised}
\@writefile{toc}{\contentsline {section}{\numberline {5}Training Setup}{14}{section.5}\protected@file@percent }
\@writefile{brf}{\backcite{wu2018unsupervised}{{14}{5}{section.5}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Cross entropy, self-supervised contrastive loss and supervised contrastive loss: The cross entropy loss (left) uses labels and a softmax loss to train a classifier; the self-supervised contrastive loss (middle) uses a contrastive loss and data augmentations to learn representations. The supervised contrastive loss (right) also learns representations using a contrastive loss, but uses label information to sample positives in addition to augmentations of the same image. Both contrastive methods can have an optional second stage which trains a model on top of the learned representations. }}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:teaser2}{{5}{14}{Cross entropy, self-supervised contrastive loss and supervised contrastive loss: The cross entropy loss (left) uses labels and a softmax loss to train a classifier; the self-supervised contrastive loss (middle) uses a contrastive loss and data augmentations to learn representations. The supervised contrastive loss (right) also learns representations using a contrastive loss, but uses label information to sample positives in addition to augmentations of the same image. Both contrastive methods can have an optional second stage which trains a model on top of the learned representations}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Gradient Derivation}{14}{section.6}\protected@file@percent }
\newlabel{sec:gradient_derivation}{{6}{14}{Gradient Derivation}{section.6}{}}
\newlabel{eqn:supp_bad_supervised_loss}{{7}{14}{Gradient Derivation}{equation.6.7}{}}
\newlabel{eqn:supp_supervised_loss}{{8}{14}{Gradient Derivation}{equation.6.8}{}}
\newlabel{eqn:supp_bad_supervised_loss_gradient}{{9}{15}{Gradient Derivation}{equation.6.9}{}}
\newlabel{eqn:supp_pip}{{10}{15}{Gradient Derivation}{equation.6.10}{}}
\citation{chen2020simple}
\newlabel{eqn:supp_supervised_loss_gradient}{{12}{16}{Gradient Derivation}{equation.6.12}{}}
\newlabel{eqn:supp_gradient}{{14}{16}{Gradient Derivation}{equation.6.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Intrinsic Hard Positive and Negative Mining Properties}{16}{section.7}\protected@file@percent }
\newlabel{sec:hard_mining_properties}{{7}{16}{Intrinsic Hard Positive and Negative Mining Properties}{section.7}{}}
\newlabel{eqn:supp_gradient_chain_rule}{{16}{17}{Intrinsic Hard Positive and Negative Mining Properties}{equation.7.16}{}}
\newlabel{eqn:supp_dz_dx}{{17}{17}{Intrinsic Hard Positive and Negative Mining Properties}{equation.7.17}{}}
\newlabel{eqn:supp_loss_gradient_pos}{{19}{17}{Intrinsic Hard Positive and Negative Mining Properties}{equation.7.19}{}}
\newlabel{eqn:supp_loss_gradient_neg}{{20}{17}{Intrinsic Hard Positive and Negative Mining Properties}{equation.7.20}{}}
\@writefile{brf}{\backcite{chen2020simple}{{17}{4}{Hfootnote.2}}}
\citation{weinberger2009distance}
\newlabel{eqn:supp_bad_supcon_factor}{{23}{18}{Intrinsic Hard Positive and Negative Mining Properties}{equation.7.23}{}}
\newlabel{eqn:supp_supcon_factor}{{24}{18}{Intrinsic Hard Positive and Negative Mining Properties}{equation.7.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Triplet Loss Derivation from Contrastive Loss}{18}{section.8}\protected@file@percent }
\newlabel{sec:triplet_loss_derivation}{{8}{18}{Triplet Loss Derivation from Contrastive Loss}{section.8}{}}
\@writefile{brf}{\backcite{weinberger2009distance}{{18}{8}{section.8}}}
\citation{chen2020simple}
\citation{schroff2015facenet}
\citation{sohn2016improved}
\@writefile{brf}{\backcite{chen2020simple}{{19}{8}{equation.8.25}}}
\@writefile{brf}{\backcite{schroff2015facenet}{{19}{8}{equation.8.25}}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Supervised Contrastive Loss Hierarchy}{19}{section.9}\protected@file@percent }
\newlabel{eqn:supp_supervised_loss}{{25}{19}{Supervised Contrastive Loss Hierarchy}{equation.9.25}{}}
\newlabel{eqn:supp_self_loss}{{26}{19}{Supervised Contrastive Loss Hierarchy}{equation.9.26}{}}
\@writefile{brf}{\backcite{sohn2016improved}{{19}{9}{equation.9.26}}}
\newlabel{eqn:supp_npairs_loss}{{27}{19}{Supervised Contrastive Loss Hierarchy}{equation.9.27}{}}
\citation{chen2020simple}
\citation{tian2019contrastive}
\citation{schroff2015facenet}
\citation{chen2020simple}
\citation{chen2020simple}
\citation{chen2020simple}
\citation{hendrycks2019benchmarking}
\citation{hendrycks2019benchmarking}
\@writefile{toc}{\contentsline {section}{\numberline {10}Effect of Temperature in Loss Function}{20}{section.10}\protected@file@percent }
\@writefile{brf}{\backcite{chen2020simple,tian2019contrastive}{{20}{10}{section.10}}}
\@writefile{brf}{\backcite{schroff2015facenet}{{20}{2}{Item.6}}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Effect of Number of Positives}{20}{section.11}\protected@file@percent }
\@writefile{brf}{\backcite{chen2020simple}{{20}{\caption@xref {??}{ on input line 8}}{table.caption.12}}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of Top-1 accuracy variability as a function of the maximum number of positives $|P(i)|$ varies from 1 to no cap . Adding more positives benefits the final Top-1 accuracy. Note that with 1 positive, this is equivalent to the self-supervised approach of \cite  {chen2020simple} where the positive is an augmented version of the \emph  {same sample}. }}{20}{table.caption.12}\protected@file@percent }
\@writefile{brf}{\backcite{chen2020simple}{{20}{5}{table.caption.12}}}
\newlabel{tab:pos_ablation}{{5}{20}{Comparison of Top-1 accuracy variability as a function of the maximum number of positives $|P(i)|$ varies from 1 to no cap . Adding more positives benefits the final Top-1 accuracy. Note that with 1 positive, this is equivalent to the self-supervised approach of \cite {chen2020simple} where the positive is an augmented version of the \emph {same sample}}{table.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Robustness}{20}{section.12}\protected@file@percent }
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{20}{12}{section.12}}}
\citation{hendrycks2019benchmarking}
\citation{hendrycks2019benchmarking}
\citation{you2017large}
\citation{chen2020simple}
\citation{tieleman2012lecture}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Expected Calibration Error and mean top-1 accuracy at different corruption severities on ImageNet-C, on the ResNet-50 architecture (top) and ResNet-200 architecture (bottom). The contrastive loss maintains a higher accuracy over the range of corruption severities, and does not suffer from increasing calibration error, unlike the cross entropy loss.}}{21}{figure.caption.14}\protected@file@percent }
\newlabel{fig:robustness}{{6}{21}{Expected Calibration Error and mean top-1 accuracy at different corruption severities on ImageNet-C, on the ResNet-50 architecture (top) and ResNet-200 architecture (bottom). The contrastive loss maintains a higher accuracy over the range of corruption severities, and does not suffer from increasing calibration error, unlike the cross entropy loss}{figure.caption.14}{}}
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{21}{12}{section.12}}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces {\bf  {Top}}: Average Expected Calibration Error (ECE) over all the corruptions in ImageNet-C \cite  {hendrycks2019benchmarking} for a given level of severity (lower is better); {\bf  {Bottom}}: Average Top-1 Accuracy over all the corruptions for a given level of severity (higher is better).}}{21}{table.caption.13}\protected@file@percent }
\@writefile{brf}{\backcite{hendrycks2019benchmarking}{{21}{6}{table.caption.13}}}
\newlabel{tab:severity}{{6}{21}{{\bf {Top}}: Average Expected Calibration Error (ECE) over all the corruptions in ImageNet-C \cite {hendrycks2019benchmarking} for a given level of severity (lower is better); {\bf {Bottom}}: Average Top-1 Accuracy over all the corruptions for a given level of severity (higher is better)}{table.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Two stage training on Cross Entropy}{21}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Training Details}{21}{section.14}\protected@file@percent }
\citation{cubuk2019autoaugment}
\citation{cubuk2019randaugment}
\citation{chen2020simple}
\citation{tian2020makes}
\citation{cubuk2019randaugment}
\citation{chen2020simple}
\citation{lim2019fast}
\citation{tian2020makes}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Comparison between representations learnt using Supervised Contrastive and representations learnt using Cross Entropy loss with either 1 stage of training or 2 stages (representation learning followed by linear classifier).}}{22}{table.caption.15}\protected@file@percent }
\newlabel{table:xenthead}{{7}{22}{Comparison between representations learnt using Supervised Contrastive and representations learnt using Cross Entropy loss with either 1 stage of training or 2 stages (representation learning followed by linear classifier)}{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Optimizer}{22}{subsection.14.1}\protected@file@percent }
\@writefile{brf}{\backcite{you2017large}{{22}{14.1}{subsection.14.1}}}
\@writefile{brf}{\backcite{chen2020simple}{{22}{14.1}{subsection.14.1}}}
\@writefile{brf}{\backcite{tieleman2012lecture}{{22}{14.1}{subsection.14.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Results of training the ResNet-50 architecture with AutoAugment data augmentation policy for 350 epochs and then training the linear classifier for another 350 epochs. Learning rates were optimized for every optimizer while all other hyper-parameters were kept the same.}}{22}{table.caption.16}\protected@file@percent }
\newlabel{tab:opt}{{8}{22}{Results of training the ResNet-50 architecture with AutoAugment data augmentation policy for 350 epochs and then training the linear classifier for another 350 epochs. Learning rates were optimized for every optimizer while all other hyper-parameters were kept the same}{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Data Augmentation}{22}{subsection.14.2}\protected@file@percent }
\@writefile{brf}{\backcite{cubuk2019autoaugment}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{cubuk2019randaugment}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{chen2020simple}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{tian2020makes}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{cubuk2019randaugment}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{chen2020simple}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{lim2019fast}{{22}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{tian2020makes}{{22}{14.2}{subsection.14.2}}}
\citation{cubuk2019randaugment}
\citation{chen2020simple}
\citation{tian2020makes}
\citation{zhang2017mixup}
\citation{yun2019cutmix}
\citation{cubuk2019randaugment}
\@writefile{brf}{\backcite{cubuk2019randaugment}{{23}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{chen2020simple}{{23}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{tian2020makes}{{23}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{zhang2017mixup}{{23}{14.2}{subsection.14.2}}}
\@writefile{brf}{\backcite{yun2019cutmix}{{23}{14.2}{subsection.14.2}}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Combinations of different data augmentations for ResNet-50 trained with optimal set of hyper-parameters and optimizers. We observe that stacked RandAugment does consistently worse for all configurations due to lower capacity of ResNet-50 models. We also observe that for other augmentations that we get the best performance by using the same augmentations in both stages of training. }}{23}{table.caption.17}\protected@file@percent }
\newlabel{tab:aug}{{9}{23}{Combinations of different data augmentations for ResNet-50 trained with optimal set of hyper-parameters and optimizers. We observe that stacked RandAugment does consistently worse for all configurations due to lower capacity of ResNet-50 models. We also observe that for other augmentations that we get the best performance by using the same augmentations in both stages of training}{table.caption.17}{}}
\@writefile{brf}{\backcite{cubuk2019randaugment}{{23}{14.2}{table.caption.17}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Top-1 Accuracy vs RandAugment magnitude for ResNet-50 (left) and ResNet-200 (right). We see that supervised contrastive methods consistently outperform cross entropy for varying strengths of augmentation.}}{23}{figure.caption.18}\protected@file@percent }
\newlabel{fig:randaug}{{7}{23}{Top-1 Accuracy vs RandAugment magnitude for ResNet-50 (left) and ResNet-200 (right). We see that supervised contrastive methods consistently outperform cross entropy for varying strengths of augmentation}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Change Log}{24}{section.15}\protected@file@percent }
\gdef \@abspage@last{24}
